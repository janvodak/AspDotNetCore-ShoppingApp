# Microservices Observability, Resilience and Monitoring

## Table of Contents

- [Microservices Observability with Distributed Logging](#microservices-observability-with-distributed-logging)
    - [Elastic Stack Integration](#elastic-stack-integration)
        - [Benefits of Distributed Logging](#benefits-of-distributed-logging)
    - [Observability Implementation](#observability-implementation)
- [Microservices Resiliency](#microservices-resiliency)
    - [Strategies to handle partial failure](#strategies-to-handle-partial-failure)
    - [Resiliency Implementation](#resiliency-implementation)
        - [Retry Pattern in Database Migration](#retry-pattern-in-database-migration)
        - [Retry Pattern for SQL Connection](#retry-pattern-for-sql-connection)
        - [Use IHttpClientFactory to implement resilient HTTP requests with Polly-based middleware](use-ihttpclientfactory-to-implement-resilient-http-requests-with-Polly-based-middleware)

Resiliency is the ability to recover from failures and continue to function.
It isn't about avoiding failures but accepting the fact that failures will happen
and responding to them in a way that avoids downtime or data loss.
The goal of resiliency is to return the application to a fully functioning state after a failure.

It's challenging enough to design and deploy a microservices-based application.
But you also need to keep your application running in an environment where some sort of failure is certain.
Therefore, your application should be resilient. It should be designed to cope with partial failures,
like network outages or nodes or VMs crashing in the cloud.
Even microservices (containers) being moved to a different node within a cluster can cause intermittent short failures within the application.

## Microservices Observability with Distributed Logging

In a distributed microservices architecture, ensuring observability is paramount for understanding and troubleshooting system behavior.
One crucial aspect of this observability is **distributed logging**, a practice that provides a unified view of log data across various microservices.

### Elastic Stack Integration

Microservices observability solution in this projects leverages the power of the **Elastic Stack**,
a comprehensive toolset that combines Elasticsearch, Logstash, and Kibana.

- **Elasticsearch** serves as the backbone for storing and indexing log data. Its distributed,
RESTful search and analytics engine ensures fast and scalable access to logs, enabling efficient querying and analysis.
  
- **Logstash** facilitates the collection, processing, and enrichment of log data.
It acts as a powerful pipeline for ingesting logs from diverse sources,
ensuring uniformity and consistency before forwarding them to Elasticsearch.

- **Kibana** provides a user-friendly interface for visualizing and exploring log data.
Its robust dashboards and analytics tools empower developers, operators,
and other stakeholders to gain valuable insights into system performance and behavior.

### Benefits of Distributed Logging

1. **Centralized Log Storage**: With Elasticsearch, logs from various microservices are centralized,
offering a holistic view of the entire system's activities.

2. **Real-time Analysis**: Logstash processes logs in real-time, ensuring that the latest information is available
for analysis and troubleshooting.

3. **Customizable Dashboards**: Kibana's customizable dashboards allow users to create visualizations tailored to their specific needs,
making it easy to identify trends, anomalies, and potential issues.

4. **Scalability and Performance**: Elasticsearch's distributed nature ensures scalability,
making it suitable for handling large volumes of log data generated by microservices in a dynamic environment.

### Observability Implementation

Implementation in this solution of distributed logging using Elastic Stack streamlines the process of monitoring and debugging microservices.
Developers can seamlessly navigate through logs, correlate events, and troubleshoot issues,
ultimately contributing to improved system reliability and performance.

By adopting distributed logging with the Elastic Stack, there is easy way how to maintain a proactive approach to observability,
enabling quick identification and resolution of potential issues in our microservices architecture.

## Microservices Resiliency

In the realm of distributed systems, particularly in microservices architectures, the specter of partial failure looms large.
Instances of microservices or containers may fail, leading to unresponsiveness or temporary downtime.
The distributed nature of these systems can exacerbate issues, such as slow response times or network-related unavailability.
A failure in one microservice can cascade through dependencies, causing a ripple effect of partial failures.

If the one of the microservice falters during order submission,
poorly implemented client processes can result in blocked threads and a degraded user experience.
This can lead to thread exhaustion in highly scalable applications, rendering the entire application globally unresponsive.

In large microservices applications, synchronous HTTP calls between services, considered an anti-pattern, can amplify partial failures.
A system with millions of incoming calls might generate even more millions of outgoing calls to various internal microservices,
creating a web of dependencies vulnerable to intermittent failures.

To mitigate such issues, this guide advocates for asynchronous communication among internal microservices to enforce autonomy.
Furthermore, the importance of designing microservices and client applications to handle partial failures is emphasized.
Resilience becomes a key factor in building robust microservices and ensuring the overall stability of distributed systems.

### Strategies to handle partial failure

To tackle partial failures, employ effective strategies:

1. **Asynchronous Communication**: Emphasize asynchronous (message-based) communication within internal microservices to prevent long chains of synchronous HTTP calls, mitigating potential outages and promoting microservice autonomy.

2. **Retries with Exponential Backoff**: Implement retries with exponential backoff to address short and intermittent failures, especially during brief service unavailability due to network issues or microservice/container relocations. Ensure proper design with circuit breakers to avoid exacerbating ripple effects and potential Denial of Service (DoS).

3. **Work Around Network Timeouts**: Design clients to use timeouts, preventing indefinite blocking and ensuring efficient resource utilization.

4. **Circuit Breaker Pattern**: Implement the Circuit Breaker pattern to monitor and limit failed requests. If the error rate surpasses a threshold, the circuit breaker interrupts further attempts, promoting resilience. After a timeout, reattempt requests to gauge success and reset the circuit breaker.

5. **Fallbacks**: Apply fallback logic, such as returning cached data or default values, when a request fails. Suitable for queries, it adds complexity for updates or commands.

6. **Limit Queued Requests**: Set an upper bound on outstanding requests from a client microservice to a specific service. The Polly Bulkhead Isolation policy can be utilized, acting as a parallelization throttle with proactive load shedding to enhance responsiveness to certain failure scenarios.

### Resiliency Implementation

The recommended approach for resiliency is to take advantage of more advanced .NET libraries like the open-source Polly library.

Polly is a .NET library that provides resilience and transient-fault handling capabilities.
You can implement those capabilities by applying Polly policies such as Retry, Circuit Breaker, Bulkhead Isolation, Timeout, and Fallback.

#### Retry Pattern in Database Migration

The Polly library could be use to implement a retry pattern during database migration.
In the context of database operations, transient issues such as SQL exceptions can occur, leading to failed migration attempts.
To address this, the code employs Polly's `RetryPolicy`, which automatically retries the migration process in the face of transient errors.
The policy specifies parameters such as the maximum retry count and sleep duration between retries.
By encapsulating the migration within a retry policy, this code enhances the resilience of the database migration process,
mitigating potential issues caused by transient errors and ensuring the successful migration of the associated database context.

#### Retry Pattern for SQL Connection

Entity Framework (EF) Core already provides internal database connection resiliency and retry logic.
But you need to enable the Entity Framework execution strategy for each DbContext connection if you want to have resilient EF Core connections.
Connection resiliency automatically retries failed database commands.
The feature can be used with any database by supplying an "execution strategy",
which encapsulates the logic necessary to detect failures and retry commands.
EF Core providers can supply execution strategies tailored to their specific database failure conditions and optimal retry policies.

#### Use IHttpClientFactory to implement resilient HTTP requests with Polly-based middleware

The conventional use of `HttpClient` in .NET applications can lead to issues such as socket exhaustion and challenges in handling DNS changes.
To address these concerns, .NET Core introduced `IHttpClientFactory`.
This interface facilitates the configuration and creation of `HttpClient` instances through Dependency Injection (DI),
promoting efficient resource management.

**Benefits of IHttpClientFactory:**
- Centralized configuration for logical `HttpClient` objects.
- Facilitates outgoing middleware through delegating handlers and integrates with Polly-based middleware for resiliency.
- Manages the lifetime of `HttpMessageHandler` to prevent common issues associated with managing `HttpClient` lifetimes.

**Ways to Use IHttpClientFactory:**
1. **Basic Usage:** Incorporate `IHttpClientFactory` in your application.
2. **Named Clients:** Configure logical `HttpClient` instances by name.
3. **Typed Clients:** Pre-configure `HttpClient` for specific use cases.
4. **Generated Clients:** Utilize dynamically generated clients.

**Typed Clients with IHttpClientFactory:**
- Typed Clients are pre-configured `HttpClient` instances tailored for specific purposes.
- Utilize `AddHttpClient` to register Typed Clients with DI, enabling configuration of base addresses, timeouts, and resilience policies.
- Handlers from a pool are assigned to Typed Clients, optimizing resource consumption.
- Set handler lifetimes to avoid common pitfalls, with flexibility for per-client configurations.

**Implementation Steps:**
1. Register `IHttpClientFactory` in the application's DI container.
2. Define Typed Client classes that accept and utilize the injected and configured `HttpClient`.
3. Register Typed Clients with DI using `AddHttpClient`, specifying configuration details and potential resiliency policies.
4. Implement Typed Client classes in your application logic, enjoying the benefits of pre-configured and resilient `HttpClient` instances.

By embracing `IHttpClientFactory` and Typed Clients, developers can enhance the management of `HttpClient` instances,
optimize resource utilization, and seamlessly integrate resilient policies for reliable HTTP communication.

A Typed Client (some service using Http Client) is effectively a transient object, that means a new instance is created each time one is needed.
It receives a new Http Client instance each time it's constructed.
However, the HttpMessageHandler objects (used by Http Client) in the pool are the objects that are reused by multiple HttpClient instances.
